{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas入门及实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas 是基于 NumPy 的一个开源 Python 库，它被广泛用于快速分析数据，以及数据清洗和准备等工作。它的名字来源是由“ Panel data”（面板数据，一个计量经济学名词）两个单词拼成的。简单地说，你可以把 Pandas 看作是 Python 版的 Excel。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/lintel-blogs-static-files/wp-content/uploads/2019/07/14165137/pandas.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas非常酷，它能很好地处理来自一大堆各种不同来源的数据，比如 Excel 表格、CSV 文件、SQL 数据库，甚至还能处理存储在网页上的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，因为Pandas基于Numpy，所以执行效率非常快。比如把几十个结构一致的csv文件合并为200M左右的单个文件（主要数据都是字符串文本），然后在此基础上做各个数据点的分组统计与分析，执行速度是秒级别的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果安装anaconda，安装命令为：```conda install pandas```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有安装anaconda，安装命令为：```pip install pandas```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pandas数据结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series 是一种<b>一维数组</b>，和 NumPy 里的数组很相似。事实上，Series 基本上就是基于 NumPy 的数组对象来的。和 NumPy 的数组不同，Series 能为数据自定义标签，也就是索引（index），然后通过索引来访问数组中的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来实战部分，在python的世界里，numpy惯例缩写为np，pandas惯例缩写为pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:30:35.686021Z",
     "start_time": "2020-11-16T16:30:35.216156Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个Series的基本语法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```series = pd.Series(data, index)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的 data 参数可以是任意数据对象，比如字典、列表甚至是 NumPy 数组，而index 参数则是对 data 的索引值，类似字典的 key。\n",
    "\n",
    "下面这个例子里，将创建一个 Series 对象，并用字符串对数字列表进行索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:30:42.661032Z",
     "start_time": "2020-11-16T16:30:42.656055Z"
    }
   },
   "outputs": [],
   "source": [
    "countries = ['USA', 'France', 'China']\n",
    "my_data = [100, 200, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:30:44.090501Z",
     "start_time": "2020-11-16T16:30:44.067530Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(my_data, countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：请记住， index 参数是可省略的，你可以选择不输入这个参数。如果不带 index 参数，Pandas 会自动用默认 index 进行索引，类似数组，索引值是 [0, ..., len(data) - 1] ，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Numpy的array创建Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:30:46.758945Z",
     "start_time": "2020-11-16T16:30:46.755825Z"
    }
   },
   "outputs": [],
   "source": [
    "array = np.array(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:30:48.016311Z",
     "start_time": "2020-11-16T16:30:48.010328Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于字典对象创建Series <br>\n",
    "如果从一个python字典对象创建Series，Pandas会自动把字典的键值设置成Series的index，并将对应的value放在和索引对应的data里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:30:54.491944Z",
     "start_time": "2020-11-16T16:30:54.488906Z"
    }
   },
   "outputs": [],
   "source": [
    "my_dict = {'a': 50, 'b': 60, 'c' : 70, 'd' : 80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:30:57.233643Z",
     "start_time": "2020-11-16T16:30:57.227668Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与Numpy数组不同，Pandas的Series能存放各种不同类型的对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从Series里获取数据，和python字典基本一样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:31:33.884881Z",
     "start_time": "2020-11-16T16:31:33.878702Z"
    }
   },
   "outputs": [],
   "source": [
    "countries = ['USA', 'France', 'China']\n",
    "my_data = [100, 200, 300]\n",
    "series = pd.Series(my_data, countries)\n",
    "series['USA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对Series进行算术运算操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对 Series 的算术运算都是基于 index 进行的。我们可以用加减乘除（+ - * /）这样的运算符对两个 Series 进行运算，Pandas 将会根据索引 index，对相应的数据进行计算，结果将会以浮点数的形式存储，以避免丢失精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:32:19.814225Z",
     "start_time": "2020-11-16T16:32:19.809266Z"
    }
   },
   "outputs": [],
   "source": [
    "series1 = pd.Series([1, 2, 3, 4], ['London', 'HongKong', 'Shanghai', 'Shenzhen'])\n",
    "series2 = pd.Series([0, 6, 7, 8], ['London', 'Shenzhen', 'NewYork', 'Delhi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:32:35.905868Z",
     "start_time": "2020-11-16T16:32:35.900875Z"
    }
   },
   "outputs": [],
   "source": [
    "print(series1)\n",
    "print(series2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:33:04.583681Z",
     "start_time": "2020-11-16T16:33:04.550778Z"
    }
   },
   "outputs": [],
   "source": [
    "series1 - series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:33:21.958606Z",
     "start_time": "2020-11-16T16:33:21.951620Z"
    }
   },
   "outputs": [],
   "source": [
    "series1 + series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:33:23.562662Z",
     "start_time": "2020-11-16T16:33:23.555710Z"
    }
   },
   "outputs": [],
   "source": [
    "series1 * series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:33:27.399387Z",
     "start_time": "2020-11-16T16:33:27.391914Z"
    }
   },
   "outputs": [],
   "source": [
    "series1 / series2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上，如果 Pandas 在两个 Series 里找不到相同的 index，对应的位置就返回一个空值 NaN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas 的 DataFrame（数据表）是一种 2 维数据结构，数据以表格的形式存储，分成若干行和列。通过 DataFrame，你能很方便地处理数据。常见的操作比如选取、替换行或列的数据，还能重组数据表、修改索引、多重筛选等。\n",
    "\n",
    "构建一个 DataFrame 对象的基本语法如下：```DataFrame(data=[],index=[],coloumns=[])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子，我们可以创建一个 5 行 4 列的 DataFrame，并填上随机数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:34:28.323969Z",
     "start_time": "2020-11-16T16:34:28.305990Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "dataframe = pd.DataFrame(data=np.random.randint(low=1, high=10, size=(5, 4)))\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该dataframe中的每一列基本上就是一个 Series ，它们都用了同一组 index。因此，我们基本上可以把 DataFrame 理解成一组采用同样索引的 Series 的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这个例子里，我们将用许多 Series 来构建一个DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T16:34:33.570028Z",
     "start_time": "2020-11-16T16:34:33.558061Z"
    }
   },
   "outputs": [],
   "source": [
    "df = {'name': pd.Series(['Jon', 'Aaron', 'Tod'], index=['a', 'b', 'c']),\n",
    "     'age': pd.Series(['39', '28', '17', '25'], index=['a', 'b', 'c', 'd']),\n",
    "     'nationality': pd.Series(['US', 'China', 'US'], ['a', 'b', 'c'])}\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的例子是用一个字典来创建DataFrame: (需要注意的是，索引与字典中各个key对应的value的数量必须相等)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': ['Jon', 'Aaron', 'Tod'],\n",
    "     'age': ['39', '28', '17'],\n",
    "     'nationality': ['US', 'China', 'US']}\n",
    "my_df = pd.DataFrame(data, \n",
    "                     index=['Lagos', 'Dubai', 'Mumbai'])\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [{'name': 'a', 'age': 3}, {'name': 'b', 'age': 18}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(my_data, index=range(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取DataFrame中的列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要获取一列的数据，还是用中括号 [] 的方式，跟 Series 类似。比如尝试获取上面这个表中的 name 列数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们只获取一列，所以返回的就是一个 Series。可以用 type() 函数确认返回值的类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_df['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果获取多个列，那返回的就是一个 DataFrame 类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df[['name', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_df[['name', 'age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向 DataFrame 里增加数据列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个列的时候，你需要先定义这个列的数据和索引。举个栗子，比如这个 DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加数据列有两种办法：可以从头开始定义一个 pd.Series，再把它放到表中，也可以利用现有的列来产生需要的新列。比如下面两种操作："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个 Series ，并放入 'year' 列中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['year'] = pd.Series(['2016', '2017', '2018'], \n",
    "                          ['Lagos', 'Dubai', 'Mumbai'])\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从现有的列创建新列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['age_year'] = my_df['age'] + my_df['year']\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从 DataFrame 里删除行/列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想要删除某一行或一列，可以用 .drop() 函数。在使用这个函数的时候，你需要先指定具体的删除方向，axis=0 对应的是行 row，而 axis=1 对应的是列 column 。\n",
    "\n",
    "删除 'age_year' 列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.drop('age_year', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除'Dubai'行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.drop('Dubai', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.drop(index='Lagos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请务必记住，除非用户明确指定，否则在调用 .drop() 的时候，Pandas 并不会真的永久性地删除这行/列。这主要是为了防止用户误操作丢失数据。\n",
    "\n",
    "你可以通过调用 df 来确认数据的完整性。如果你确定要永久性删除某一行/列，现在my_df数据并没有发生变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你需要加上 inplace=True 参数，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.drop('Dubai', axis=0, inplace=True)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取 DataFrame 中的一行或多行数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要获取某一行，你需要用 .loc[] 来按索引（标签名）引用这一行，或者用 .iloc[]，按这行在表中的位置（行数）来引用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.loc['Lagos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.loc[['Lagos', 'Mumbai']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.iloc[[0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时你可以用 .loc[] 来指定具体的行列范围，并生成一个子数据表，就像在 NumPy里做的一样。比如，提取 'Lagos' 行中 'Name’ 列的内容，可以如下操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.loc['Lagos', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_df.loc['Lagos', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.loc['Lagos', 'name'] = 'Amanda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.loc['Lagos', 'name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，你还可以制定多行和/或多列, 此时得到的是DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.loc[['Lagos', 'Mumbai'], ['name', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_df.loc[['Lagos', 'Mumbai'], ['name', 'age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用中括号 [] 的方式，除了直接指定选中某些列外，还能接收一个条件语句，然后筛选出符合条件的行/列。比如，我们希望在下面这个表格中筛选出 'W'>0 的行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "df = pd.DataFrame(data=np.random.randn(5, 4), \n",
    "                  index=['a', 'b', 'c', 'd', 'e'], \n",
    "                  columns=['W', 'X', 'Y', 'Z'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['X'] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.X > 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只看W列中，X > 0.5的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['X'] > 0.5]['W']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似的，你还可以试试这样的语句 ```df[df['X']>0.5][['X','Y']]``` ，结果将会是这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['X']>0.5][['X','Y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相当于将下面操作连在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myseries = df['X'] > 0.5\n",
    "result = df[myseries]\n",
    "mycols = ['X', 'Y']\n",
    "result[mycols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以用逻辑运算符 &（与）和 |（或）来链接多个条件语句，以便一次应用多个筛选条件到当前的 DataFrame 上。举个栗子，你可以用下面的方法筛选出同时满足 'W'>0 和'X'>1 的行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~((df['W'] > 0) | (df['X'] > 0.5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['W'] > 0) | (df['X'] > 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重置 DataFrame 的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你觉得当前 DataFrame 的索引有问题，你可以用 .reset_index() 简单地把整个表的索引都重置掉。这个方法将把目标 DataFrame 的索引保存在一个叫 index 的列中，而把表格的索引变成默认的从零开始的数字，也就是 [0, ..., len(data) - 1] 。比如下面这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和删除操作差不多，.reset_index() 并不会永久改变你表格的索引，除非你调用的时候明确传入了 inplace 参数，比如：.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：原有的索引会作为一个备份列，列名为index, 出现在DataFrame中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置 DataFrame 的索引值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，我们还可以用 .set_index() 方法，将 DataFrame 里的某一列作为索引来用。比如，我们在这个表里新建一个名为 \"ID\" 的列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table的列名\n",
    "columns = list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table的行数量\n",
    "len(df)\n",
    "for i in range(len(df)):\n",
    "    print(df.loc[i, columns[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'] = ['{0}{1}'.format('myid',i) for i in range(len(df))]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的，这种做法也不会直接修改df的索引，依然需要设置inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    print(df.loc['myid{0}'.format(i), columns[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：set_index，不会保留原有索引的备份，会直接将原有索引替代为新的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多级索引（MultiIndex）以及命名索引的不同等级"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多级索引其实就是一个由元组（Tuple）组成的数组，每一个元组都是独一无二的。你可以从一个包含许多数组的列表中创建多级索引（调用 MultiIndex.from_arrays ），也可以用一个包含许多元组的数组（调用 MultiIndex.from_tuples ）或者是用一对可迭代对象的集合（比如两个列表，互相两两配对）来构建（调用MultiIndex.from_product ）。\n",
    "\n",
    "下面这个例子，我们从元组中创建多级索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside = ['0 level', '0 level', '0 level', 'A level', 'A level', 'A level',]\n",
    "inside = [21, 22, 23, 21, 22, 23]\n",
    "myindex = list(zip(outside, inside))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后这个 list(zip()) 的嵌套函数，把上面两个列表合并成了一个每个元素都是元组的列表。这时 my_index 的内容如上所示。\n",
    "\n",
    "接下来，我们调用 .MultiIndex.from_tuples(my_index) 生成一个多级索引对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myindex = pd.MultiIndex.from_tuples(myindex)\n",
    "myindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(myindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，将这个多级索引对象转成一个 DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "df = pd.DataFrame(np.random.randn(6, 2), index=myindex, columns=['A', 'B'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要获取多级索引中的数据，还是用到 .loc[] 。比如，先获取 'O Level' 下的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['0 level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后再用一次 .loc[]，获取下一层 21 里的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['0 level'].loc[21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，df 这个 DataFrame 的头两个索引列没有名字，看起来不太易懂。我们可以用 .index.names 给它们加上名字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.names = ['level', 'num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉选择行和列中的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以用 .xs() 方法轻松获取到多级索引中某些特定级别的数据。比如，我们需要找到所有 Levels 中，Num = 22 的行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.xs(22, level='num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 清洗数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 删除或填充空值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在许多情况下，如果你用 Pandas 来读取大量数据，往往会发现原始数据中会存在不完整的地方。在 DataFrame 中缺少数据的位置， Pandas 会自动填入一个空值，比如 NaN或 Null 。因此，我们可以选择用 .dropna() 来丢弃这些自动填充的值，或是用.fillna() 来自动给这些空值填充数据。\n",
    "\n",
    "比如这个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {'A':[1, np.nan, 3, 4, np.nan, np.nan, 6, 7, 8, np.nan, np.nan],\n",
    "      'B':[2, np.nan, 3, 4, 5, 17, 6, 7, 8, 20, np.nan], \n",
    "      'C':[4, 5, 3, 4, 5, 25, 6, 7, 8, np.nan, np.nan]}\n",
    "df = pd.DataFrame(dt)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你使用 .dropna() 方法时，就是告诉 Pandas 删除掉存在一个或多个空值的行（或者列）。删除列用的是 .dropna(axis=1) ，删除行用的是 .dropna(axis=0) 。\n",
    "\n",
    "请注意，如果你没有指定 axis 参数，默认是删除行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 删除行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：DataFrame的删除操作，并不会永久删除数据，依然需要加入inplace=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 删除列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似的，如果你使用 .fillna() 方法，Pandas 将对这个 DataFrame 里所有的空值位置填上你指定的默认值。比如，将表中所有 NaN 替换成 20 ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，这有的时候打击范围太大了。于是我们可以选择只对某些特定的行或者列进行填充。比如只对 'B' 列进行操作，在空值处填入该列的平均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先这里是回复同学的问题，如果遇到空值，希望对空值按照其下方的值直接填充，如果没有才填默认值？<br>\n",
    "下面的例子，是实现方法，如果后续行没有值了，则用0填充。<br>\n",
    "但是这种做法，会真正修改A列的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['A'].isnull()\n",
    "nanlist = []\n",
    "foundnan = False\n",
    "for i, x in enumerate(a):\n",
    "    if x:\n",
    "        nanlist.append(i)\n",
    "        foundnan = True\n",
    "    elif foundnan:\n",
    "        if len(nanlist) > 0:\n",
    "            df['A'][nanlist] = df['A'][i]\n",
    "        foundnan = False\n",
    "        nanlist = []\n",
    "\n",
    "if foundnan and len(nanlist) > 0:\n",
    "    df['A'][nanlist] = 0\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['B'].fillna(df['B'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，'B' 列的平均值是 8.0，所以第二行的空值被填上了 8.0。\n",
    "\n",
    "同理，.dropna() 和 .fillna() 并不会永久性改变你的数据，除非你传入了inplace=True 参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 查找两列不相同的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'docid': [1, 2, 3, 5, 6]})\n",
    "df2 = pd.DataFrame({'docid': [1, 2, 5, 6]})\n",
    "np.setdiff1d(df1['docid'], df2['docid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分组统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas 的分组统计功能可以按某一列的内容对数据行进行分组，并对其应用统计函数，比如求和，平均数，中位数，标准差等等…\n",
    "\n",
    "举例来说，用 .groupby() 方法，我们可以对下面这数据表按 'Company' 列进行分组，并用 .mean() 求每组的平均值：\n",
    "\n",
    "首先，初始化一个DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {'company':['a', 'a', 'a', 'b', 'b'], \n",
    "      'person': ['Ali', 'Bale', 'Alice', 'Charlie', 'David'], \n",
    "      'sale': [105, 126, 158, 321, 256], 'asset': [1045, 1268, 2053, 3158, 2890]}\n",
    "df = pd.DataFrame(dt)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据company分组，求所有的数字列求均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('company').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 .count() 方法，能对 DataFrame 中的某个元素出现的次数进行计数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与Numpy类似，Pandas也有多个描述性的统计指标方法。\n",
    "\n",
    "Pandas 的 .describe() 方法将对 DataFrame 里的数据进行分析，并一次性生成多个描述性的统计指标，方便用户对数据有一个直观上的认识。\n",
    "\n",
    "对于非分组列，将统计该列的条目，唯一值的数量，出现频率最多的值，以及出现频率数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于分组数据，生成的指标，从左到右分别是：计数、平均数、标准差、最小值、25% 50% 75% 位置的值、最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('company').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你不喜欢这个排版，你可以用 .transpose() 方法获得一个竖排的格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('company').describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以只统计某个数据项(将获得Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('company').describe().transpose()['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 堆叠(Concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "堆叠基本上就是简单地把多个 DataFrame 堆在一起，拼成一个更大的 DataFrame。当你进行堆叠的时候，请务必注意你数据表的索引和列的延伸方向，堆叠的方向要和它一致。\n",
    "\n",
    "比如，有这样3个 DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data=[['A0', 'B0', 'C0', 'D0'], \n",
    "                         ['A1', 'B1', 'C1', 'D1'], \n",
    "                         ['A2', 'B2', 'C2', 'D2'], \n",
    "                         ['A3', 'B3', 'C3', 'D3'],\n",
    "                         ['A31', 'B31', 'C31', 'D31']], \n",
    "                   columns=['A', 'B', 'C', 'D'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data=[['A4', 'B4', 'C4', 'D4'], \n",
    "                         ['A5', 'B5', 'C5', 'D5'], \n",
    "                         ['A6', 'B6', 'C6', 'D6'], \n",
    "                         ['A7', 'B7', 'C7', 'D7']], \n",
    "                   columns=['A', 'B', 'C', 'D'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(data=[['A8', 'B8', 'C8', 'D8'], \n",
    "                         ['A9', 'B9', 'C9', 'D9'], \n",
    "                         ['A10', 'B10', 'C10', 'D10'], \n",
    "                         ['A11', 'B11', 'C11', 'D11']], \n",
    "                   columns=['A', 'B', 'C', 'D'])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffull = pd.concat([df1, df2, df3])\n",
    "# dffull.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffull.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffull.sort_values(['A', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffull.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们没有指定堆叠的方向，Pandas 默认按行的方向堆叠，把每个表的索引按顺序叠加。\n",
    "\n",
    "如果你想要按列的方向堆叠，那你需要传入 axis=1 参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2, df3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，这里出现了一大堆空值。因为我们用来堆叠的3个 DataFrame 里，有许多索引是没有对应数据的。因此，当你使用 pd.concat() 的时候，一定要注意堆叠方向的坐标轴（行或列）含有所需的所有数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 归并(Merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 pd.merge() 函数，能将多个 DataFrame 归并在一起，它的合并方式类似合并 SQL 数据表的方式。\n",
    "\n",
    "归并操作的基本语法是 pd.merge(left, right, how='inner', on='Key') 。其中 left 参数代表放在左侧的 DataFrame，而 right 参数代表放在右边的 DataFrame；how='inner' 指的是当左右两个 DataFrame 中存在不重合的 Key 时，取结果的方式：inner 代表交集；Outer 代表并集。最后，on='Key' 代表需要合并的键值所在的列，最后整个表格会以该列为准进行归并。\n",
    "\n",
    "对于两个都含有 key 列的 DataFrame，我们可以这样归并："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4'],\n",
    "                    'A': ['A0', 'A1', 'A2', 'A3', 'A4'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3', 'B4']})\n",
    "right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, how='inner', on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, how='outer', on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，我们可以传入多个 on 参数，这样就能按多个键值进行归并："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2', 'K3'],\n",
    "                     'key2': ['K0', 'K1', 'K2', 'K3', 'K4'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3', 'A4'],\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3', 'B4']})\n",
    "right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "                      'key2': ['K0', 'K0', 'K0', 'K0'],\n",
    "                      'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                      'D': ['D0', 'D1', 'D2', 'D3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=['key1', 'key2'])\n",
    "# select * from left \n",
    "# inner join on left.key1 = right.key1 and left.key2 = right.key2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=['key1', 'key2'], how='outer')\n",
    "# select * from left \n",
    "# (left + right) join on left.key1 = right.key1 and left.key2 = right.key2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 连接(Join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你要把两个表连在一起，然而它们之间没有太多共同的列，那么你可以试试 .join() 方法。和 .merge() 不同，连接采用索引作为公共的键，而不是某一列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3', 'A4'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3', 'B4']}, index=['2000', '2001', '2006', '2003', '2004'])\n",
    "right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']}, index=['2000', '2007', '2003', '2004'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，inner 代表交集，Outer 代表并集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 查找不重复的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不重复的值，在一个 DataFrame 里往往是独一无二，与众不同的。找到不重复的值，在数据分析中有助于避免样本偏差。在 Pandas 里，主要用到 3 种方法：\n",
    "\n",
    "首先是 .unique() 方法。比如在下面这个 DataFrame 里，查找 col2 列中所有不重复的值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'col1': [1, 2, 3, 4], \n",
    "                   'col2': [444, 555, 666, 444], \n",
    "                   'col3': ['abcd', 'def', 'ghijklm', 'xyz']})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col2'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了列出所有不重复的值，我们还能用 .nunique() 方法，获取所有不重复值的个数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col2'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，还可以用 .value_counts() 同时获得所有值和对应值的计数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 删除重复行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'a': [1,2,3,4,1], 'b': [1,2,15,18,1]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "索引0与4对应的行的数据是完全一样的，如果需要去除，则使用下面的语句："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply()方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 .apply() 方法，可以对 DataFrame 中的数据应用自定义函数，进行数据处理。比如，我们先定义一个 square() 函数，然后对表中的 col1 列应用这个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1': [1, 2, 3, 4], \n",
    "                   'col2': [444, 555, 666, 444], \n",
    "                   'col3': ['abcd', 'def', 'ghijklm', 'xyz']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "df['col1'].apply(square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面这个例子中，这个函数被应用到这一列里的每一个元素上。同样，我们也可以调用任意的内置函数。比如对 col3 列取长度 len ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col3'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有的时候，你定义了一个函数，而它其实只会被用到一次。那么，我们可以用 lambda 表达式来代替函数定义，简化代码。比如，我们可以用这样的 lambda 表达式代替上面的函数定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col1'].apply(lambda x : x * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取DataFrame的属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 的属性包括列和索引的名字。假如你不确定表中的某个列名是否含有空格之类的字符，你可以通过 .columns 来获取属性值，以查看具体的列名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要将整个表按某一列的值进行排序，可以用 .sort_values() ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_df = df.sort_values('col2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，表格变成按 col2 列的值从小到大排序。要注意的是，表格的索引 index 还是对应着排序前的行，并没有因为排序而丢失原来的索引数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查找空值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如你有一个很大的数据集，你可以用 Pandas 的 .isnull() 方法，方便快捷地发现表中的空值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()['col1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这返回的是一个新的 DataFrame，里面用布尔值（True/False）表示原 DataFrame 中对应位置的数据是否是空值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据透视表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用 Excel 的时候，你或许已经试过数据透视表的功能了。数据透视表是一种汇总统计表，它展现了原表格中数据的汇总统计结果。Pandas 的数据透视表能自动帮你对数据进行分组、切片、筛选、排序、计数、求和或取平均值，并将结果直观地显示出来。比如，这里有个关于动物的统计表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'a': ['dog','dog','dog','goat','goat','goat'],\n",
    "        'b': ['brown','brown','black','black','brown','black'],\n",
    "        'c': ['x','y','x','y','x','y'],\n",
    "        'd': [1,3,2,5,4,1]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas 数据透视表的语法是 .pivot_table(data, values='', index=[''], columns=['']) ，其中 values 代表我们需要汇总统计的数据点所在的列，index 表示按该列进行分组索引，而 columns 则表示最后结果将按该列的数据进行分列。你可以在 Pandas 的官方文档 中找到更多数据透视表的详细用法和例子。\n",
    "\n",
    "于是，我们按上面的语法，给这个动物统计表创建一个数据透视表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pd.pivot_table(df, values='d', index=['a','b'], columns='c',aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以直接对DataFrame对象做操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(values='d', index=['a','b'], columns='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，数据透视表的某些位置是 NaN 空值，因为在原数据里没有对应的条件下的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 导入导出数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以这样理解，array, dictionary, csv，excel，数据库等结构化数据载体，都是DataFrame的数据源，只是加载的方法有一些区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从数据库读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里只是举了如何从sql server获取数据，大家知道这种做法即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```with pymssql.connect(server=\"myserver\",\n",
    "                         user=\"myuser\",\n",
    "                         password=\"mypwd\",\n",
    "                         database=\"mydb\") as conn:\n",
    "    df = pd.read_sql('SELECT * FROM MYTABLE', con=conn)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从CSV读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单地说，只要用 pd.read_csv() 就能将 CSV 文件里的数据转换成 DataFrame 对象："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，尽量加上encoding的值，避免读取到乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./csv/animal_2019.csv', encoding='utf-8').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有时操作字符集特别丰富的文本，并尝试保存为csv，常常保存为utf-16模式，而且用tab作为分隔符，如: <br>\n",
    "```dfsample.to_csv(file_path, encoding='utf-16', sep=\"\\t\")```<br>\n",
    "那么读取的时候，也应该保持同样的编码与分隔符：<br>\n",
    "```pd.read_csv(file, encoding=\"utf-16\", sep=\"\\t\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'a': ['dog','dog','dog','goat','goat','goat'],\n",
    "        'b': ['brown','brown','black','black','brown','black'],\n",
    "        'c': ['x','y','x','y','x','y'],\n",
    "        'd': [1,3,2,5,4,1]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./csv/animal_2019.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里传入 index=False 参数是因为不希望 Pandas 把索引列的 0~5 也存到文件中。\n",
    "\n",
    "为了确保数据已经保存好了，你可以试试用 pd.read_csv('New_dataframe') ，把这个文件的内容读取出来看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./csv/animal_2019.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 写入Excel表格文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟写入 CSV 文件类似，我们可以将一个 DataFrame 对象存成 .xlsx 文件，语法是 .to_excel() ："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过前提是需要安装openpyxl，通过```pip install openpyxl```即可完成安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('./excel/animal_2019.xlsx', encoding='utf-8', sheet_name='animal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 写入多个sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方用法\n",
    ">```\n",
    ">>> writer = pd.ExcelWriter('output.xlsx')\n",
    ">>> df1.to_excel(writer,'Sheet1')\n",
    ">>> df2.to_excel(writer,'Sheet2')\n",
    ">>> writer.save()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df1,df2均为sql查询来的数据\n",
    "excel_filepath为要生成保存的excel文件地址\n",
    "\"\"\"\n",
    "excel_filepath = './excel/multisheets.xlsx'\n",
    "write = pd.ExcelWriter(excel_filepath)\n",
    "data = {'a': ['dog','dog','dog','goat','goat','goat'],\n",
    "        'b': ['brown','brown','black','black','brown','black'],\n",
    "        'c': ['x','y','x','y','x','y'],\n",
    "        'd': [1,3,2,5,4,1]}\n",
    "df1 = pd.DataFrame(data)\n",
    "# df1 = pd.DataFrame(df1)\n",
    "excel_header = ['a','b','c','d']#excel的标题\n",
    "df1.to_excel(write,sheet_name='Sheet1',header=excel_header,index=False)\n",
    "\n",
    "data = {'a': ['duck','duck','duck','sheep','sheep','sheep'],\n",
    "        'b': ['brown','brown','black','black','brown','black'],\n",
    "        'c': ['x','y','x','y','x','y'],\n",
    "        'd': [1,3,2,5,4,1]}\n",
    "df2 = pd.DataFrame(data)\n",
    "# df2 = pd.DataFrame(df2)\n",
    "excel_header = ['a','b','c','d']\n",
    "df2.to_excel(write,sheet_name='Sheet2',header=excel_header,index=False)\n",
    "write.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取 Excel 表格文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel 文件是一个不错的数据来源。使用 pd.read_excel() 方法，我们能将 Excel 表格中的数据导入 Pandas 中。请注意，Pandas 只能导入表格文件中的数据，其他对象，例如宏、图形和公式等都不会被导入。如果文件中存在有此类对象，可能会导致 pd.read_excel() 方法执行失败。\n",
    "\n",
    "举个例子，假设我们有一个 Excel 表格 './excel/animal.xlsx'，然后读取它的数据："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取Excel的前提是安装xlrd模块: ```pip install xlrd```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('./excel/animal.xlsx', encoding='utf-8', sheet_name='animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('./excel/multisheets.xlsx', encoding='utf-8', sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('./excel/multisheets.xlsx', encoding='utf-8', sheet_name='Sheet2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取 HTML 文件中的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了读取 HTML 文件，你需要安装 lxml 以及 BeautifulSoup4 库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子，我们用让 Pandas 读取这个页面的数据： ./html/167711672.htm。由于一个页面上含有多个不同的表格，我们需要通过下标 [0, ..., len(tables) - 1] 访问数组中的不同元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = pd.read_html('./html/167711672.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "for index, df in enumerate(dflist):\n",
    "    print('Show the {0} table'.format(index))\n",
    "    display(df.head())\n",
    "    if index > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist[8][0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
